{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ruta al archivo CSV\n",
    "ruta_archivo = '/home/azureuser/cloudfiles/code/Users/jordanperez/Grouped_datasets/dataset_1.csv'\n",
    "\n",
    "# Importar el archivo CSV utilizando pandas\n",
    "datos = pd.read_csv(ruta_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.48828125</th>\n",
       "      <th>0.9765625</th>\n",
       "      <th>1.46484375</th>\n",
       "      <th>1.953125</th>\n",
       "      <th>2.44140625</th>\n",
       "      <th>2.9296875</th>\n",
       "      <th>3.41796875</th>\n",
       "      <th>3.90625</th>\n",
       "      <th>4.39453125</th>\n",
       "      <th>...</th>\n",
       "      <th>495.60546875</th>\n",
       "      <th>496.09375</th>\n",
       "      <th>496.58203125</th>\n",
       "      <th>497.0703125</th>\n",
       "      <th>497.55859375</th>\n",
       "      <th>498.046875</th>\n",
       "      <th>498.53515625</th>\n",
       "      <th>499.0234375</th>\n",
       "      <th>499.51171875</th>\n",
       "      <th>500.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.258129e-08</td>\n",
       "      <td>7.964380e-08</td>\n",
       "      <td>1.560436e-08</td>\n",
       "      <td>1.008043e-08</td>\n",
       "      <td>1.047214e-08</td>\n",
       "      <td>7.753222e-09</td>\n",
       "      <td>5.599398e-09</td>\n",
       "      <td>5.683120e-09</td>\n",
       "      <td>9.217123e-09</td>\n",
       "      <td>7.228860e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>7.082812e-09</td>\n",
       "      <td>7.629753e-09</td>\n",
       "      <td>4.964598e-09</td>\n",
       "      <td>6.459483e-09</td>\n",
       "      <td>9.117289e-09</td>\n",
       "      <td>1.096673e-08</td>\n",
       "      <td>1.372002e-08</td>\n",
       "      <td>5.261354e-09</td>\n",
       "      <td>2.566851e-09</td>\n",
       "      <td>1.710661e-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.565524e-09</td>\n",
       "      <td>2.409528e-08</td>\n",
       "      <td>2.466291e-09</td>\n",
       "      <td>1.764895e-09</td>\n",
       "      <td>1.548712e-09</td>\n",
       "      <td>9.412745e-10</td>\n",
       "      <td>7.866185e-10</td>\n",
       "      <td>3.795916e-10</td>\n",
       "      <td>5.018929e-10</td>\n",
       "      <td>9.735305e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>7.100681e-10</td>\n",
       "      <td>5.651571e-10</td>\n",
       "      <td>5.131621e-10</td>\n",
       "      <td>4.374353e-10</td>\n",
       "      <td>5.290657e-10</td>\n",
       "      <td>4.596035e-10</td>\n",
       "      <td>7.313141e-10</td>\n",
       "      <td>8.308055e-10</td>\n",
       "      <td>5.550643e-10</td>\n",
       "      <td>2.333624e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.721098e-09</td>\n",
       "      <td>3.337447e-08</td>\n",
       "      <td>7.372148e-09</td>\n",
       "      <td>4.265692e-09</td>\n",
       "      <td>5.081157e-09</td>\n",
       "      <td>4.336692e-09</td>\n",
       "      <td>2.269700e-09</td>\n",
       "      <td>3.195305e-09</td>\n",
       "      <td>4.462770e-09</td>\n",
       "      <td>3.788782e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>2.239325e-09</td>\n",
       "      <td>2.574185e-09</td>\n",
       "      <td>2.859026e-09</td>\n",
       "      <td>3.416060e-09</td>\n",
       "      <td>4.222006e-09</td>\n",
       "      <td>4.185060e-09</td>\n",
       "      <td>7.010891e-09</td>\n",
       "      <td>5.936980e-09</td>\n",
       "      <td>1.659414e-09</td>\n",
       "      <td>7.369086e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.294428e-09</td>\n",
       "      <td>2.363276e-08</td>\n",
       "      <td>5.444321e-09</td>\n",
       "      <td>1.466916e-09</td>\n",
       "      <td>2.093515e-09</td>\n",
       "      <td>3.059440e-09</td>\n",
       "      <td>1.631231e-09</td>\n",
       "      <td>1.012918e-09</td>\n",
       "      <td>9.964329e-10</td>\n",
       "      <td>1.097776e-09</td>\n",
       "      <td>...</td>\n",
       "      <td>5.067161e-09</td>\n",
       "      <td>3.771930e-09</td>\n",
       "      <td>3.589092e-09</td>\n",
       "      <td>1.680800e-09</td>\n",
       "      <td>2.301844e-09</td>\n",
       "      <td>2.628996e-09</td>\n",
       "      <td>2.972329e-09</td>\n",
       "      <td>1.644647e-09</td>\n",
       "      <td>2.010561e-09</td>\n",
       "      <td>8.164513e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.310373e-05</td>\n",
       "      <td>1.910161e-04</td>\n",
       "      <td>7.863728e-05</td>\n",
       "      <td>1.650860e-05</td>\n",
       "      <td>2.493174e-05</td>\n",
       "      <td>3.157603e-05</td>\n",
       "      <td>1.731895e-05</td>\n",
       "      <td>9.630589e-06</td>\n",
       "      <td>5.099844e-06</td>\n",
       "      <td>1.183991e-05</td>\n",
       "      <td>...</td>\n",
       "      <td>7.750843e-05</td>\n",
       "      <td>7.238917e-05</td>\n",
       "      <td>5.779691e-05</td>\n",
       "      <td>4.722037e-05</td>\n",
       "      <td>3.419079e-05</td>\n",
       "      <td>8.060802e-05</td>\n",
       "      <td>9.198100e-05</td>\n",
       "      <td>5.138549e-05</td>\n",
       "      <td>1.361862e-04</td>\n",
       "      <td>1.027223e-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25039</th>\n",
       "      <td>1.184476e-05</td>\n",
       "      <td>4.308472e-05</td>\n",
       "      <td>1.555046e-05</td>\n",
       "      <td>7.743723e-06</td>\n",
       "      <td>4.632000e-06</td>\n",
       "      <td>2.360844e-06</td>\n",
       "      <td>2.578222e-06</td>\n",
       "      <td>2.259323e-06</td>\n",
       "      <td>1.219317e-06</td>\n",
       "      <td>1.396780e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.048813e-08</td>\n",
       "      <td>1.220443e-08</td>\n",
       "      <td>2.558777e-08</td>\n",
       "      <td>2.637820e-08</td>\n",
       "      <td>2.953406e-08</td>\n",
       "      <td>4.402479e-08</td>\n",
       "      <td>3.991601e-08</td>\n",
       "      <td>2.014788e-08</td>\n",
       "      <td>1.905162e-08</td>\n",
       "      <td>1.079713e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25040</th>\n",
       "      <td>1.452807e-05</td>\n",
       "      <td>2.124202e-04</td>\n",
       "      <td>3.548324e-05</td>\n",
       "      <td>1.883042e-05</td>\n",
       "      <td>1.430048e-05</td>\n",
       "      <td>6.398688e-06</td>\n",
       "      <td>6.832693e-06</td>\n",
       "      <td>5.207948e-06</td>\n",
       "      <td>2.063769e-06</td>\n",
       "      <td>2.938399e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.543073e-06</td>\n",
       "      <td>1.540405e-06</td>\n",
       "      <td>4.143394e-06</td>\n",
       "      <td>4.724783e-06</td>\n",
       "      <td>1.508944e-06</td>\n",
       "      <td>7.472969e-07</td>\n",
       "      <td>8.472217e-07</td>\n",
       "      <td>1.262639e-06</td>\n",
       "      <td>1.301447e-06</td>\n",
       "      <td>6.222116e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25041</th>\n",
       "      <td>2.123605e-05</td>\n",
       "      <td>1.248198e-04</td>\n",
       "      <td>3.587728e-05</td>\n",
       "      <td>2.228530e-05</td>\n",
       "      <td>1.187294e-05</td>\n",
       "      <td>6.733346e-06</td>\n",
       "      <td>5.613463e-06</td>\n",
       "      <td>6.239971e-06</td>\n",
       "      <td>5.315463e-06</td>\n",
       "      <td>5.999354e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.119807e-06</td>\n",
       "      <td>1.080242e-06</td>\n",
       "      <td>1.383162e-06</td>\n",
       "      <td>1.045849e-06</td>\n",
       "      <td>5.055832e-07</td>\n",
       "      <td>7.280705e-07</td>\n",
       "      <td>1.121674e-06</td>\n",
       "      <td>1.241047e-06</td>\n",
       "      <td>7.918726e-07</td>\n",
       "      <td>3.048470e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25042</th>\n",
       "      <td>1.731580e-05</td>\n",
       "      <td>7.642855e-05</td>\n",
       "      <td>4.322482e-05</td>\n",
       "      <td>2.030433e-05</td>\n",
       "      <td>1.863112e-05</td>\n",
       "      <td>9.599866e-06</td>\n",
       "      <td>7.626317e-06</td>\n",
       "      <td>3.199077e-06</td>\n",
       "      <td>2.288714e-06</td>\n",
       "      <td>5.547822e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.302529e-06</td>\n",
       "      <td>1.527490e-06</td>\n",
       "      <td>2.361578e-06</td>\n",
       "      <td>2.126980e-06</td>\n",
       "      <td>1.410627e-06</td>\n",
       "      <td>7.396353e-07</td>\n",
       "      <td>7.730455e-07</td>\n",
       "      <td>9.197263e-07</td>\n",
       "      <td>9.801953e-07</td>\n",
       "      <td>5.919500e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25043</th>\n",
       "      <td>1.361369e-05</td>\n",
       "      <td>3.747419e-04</td>\n",
       "      <td>6.118349e-05</td>\n",
       "      <td>1.884354e-05</td>\n",
       "      <td>1.550142e-05</td>\n",
       "      <td>1.046569e-05</td>\n",
       "      <td>1.345200e-05</td>\n",
       "      <td>1.093485e-05</td>\n",
       "      <td>8.324917e-06</td>\n",
       "      <td>3.879578e-06</td>\n",
       "      <td>...</td>\n",
       "      <td>1.810441e-06</td>\n",
       "      <td>5.353157e-07</td>\n",
       "      <td>9.972832e-07</td>\n",
       "      <td>1.179421e-06</td>\n",
       "      <td>6.795099e-07</td>\n",
       "      <td>9.527096e-07</td>\n",
       "      <td>5.708839e-07</td>\n",
       "      <td>6.369546e-07</td>\n",
       "      <td>6.318687e-07</td>\n",
       "      <td>3.801804e-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25044 rows × 1025 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0.0    0.48828125     0.9765625    1.46484375      1.953125   \n",
       "0      1.258129e-08  7.964380e-08  1.560436e-08  1.008043e-08  1.047214e-08  \\\n",
       "1      1.565524e-09  2.409528e-08  2.466291e-09  1.764895e-09  1.548712e-09   \n",
       "2      6.721098e-09  3.337447e-08  7.372148e-09  4.265692e-09  5.081157e-09   \n",
       "3      2.294428e-09  2.363276e-08  5.444321e-09  1.466916e-09  2.093515e-09   \n",
       "4      2.310373e-05  1.910161e-04  7.863728e-05  1.650860e-05  2.493174e-05   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "25039  1.184476e-05  4.308472e-05  1.555046e-05  7.743723e-06  4.632000e-06   \n",
       "25040  1.452807e-05  2.124202e-04  3.548324e-05  1.883042e-05  1.430048e-05   \n",
       "25041  2.123605e-05  1.248198e-04  3.587728e-05  2.228530e-05  1.187294e-05   \n",
       "25042  1.731580e-05  7.642855e-05  4.322482e-05  2.030433e-05  1.863112e-05   \n",
       "25043  1.361369e-05  3.747419e-04  6.118349e-05  1.884354e-05  1.550142e-05   \n",
       "\n",
       "         2.44140625     2.9296875    3.41796875       3.90625    4.39453125   \n",
       "0      7.753222e-09  5.599398e-09  5.683120e-09  9.217123e-09  7.228860e-09  \\\n",
       "1      9.412745e-10  7.866185e-10  3.795916e-10  5.018929e-10  9.735305e-10   \n",
       "2      4.336692e-09  2.269700e-09  3.195305e-09  4.462770e-09  3.788782e-09   \n",
       "3      3.059440e-09  1.631231e-09  1.012918e-09  9.964329e-10  1.097776e-09   \n",
       "4      3.157603e-05  1.731895e-05  9.630589e-06  5.099844e-06  1.183991e-05   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "25039  2.360844e-06  2.578222e-06  2.259323e-06  1.219317e-06  1.396780e-06   \n",
       "25040  6.398688e-06  6.832693e-06  5.207948e-06  2.063769e-06  2.938399e-06   \n",
       "25041  6.733346e-06  5.613463e-06  6.239971e-06  5.315463e-06  5.999354e-06   \n",
       "25042  9.599866e-06  7.626317e-06  3.199077e-06  2.288714e-06  5.547822e-06   \n",
       "25043  1.046569e-05  1.345200e-05  1.093485e-05  8.324917e-06  3.879578e-06   \n",
       "\n",
       "       ...  495.60546875     496.09375  496.58203125   497.0703125   \n",
       "0      ...  7.082812e-09  7.629753e-09  4.964598e-09  6.459483e-09  \\\n",
       "1      ...  7.100681e-10  5.651571e-10  5.131621e-10  4.374353e-10   \n",
       "2      ...  2.239325e-09  2.574185e-09  2.859026e-09  3.416060e-09   \n",
       "3      ...  5.067161e-09  3.771930e-09  3.589092e-09  1.680800e-09   \n",
       "4      ...  7.750843e-05  7.238917e-05  5.779691e-05  4.722037e-05   \n",
       "...    ...           ...           ...           ...           ...   \n",
       "25039  ...  1.048813e-08  1.220443e-08  2.558777e-08  2.637820e-08   \n",
       "25040  ...  1.543073e-06  1.540405e-06  4.143394e-06  4.724783e-06   \n",
       "25041  ...  1.119807e-06  1.080242e-06  1.383162e-06  1.045849e-06   \n",
       "25042  ...  1.302529e-06  1.527490e-06  2.361578e-06  2.126980e-06   \n",
       "25043  ...  1.810441e-06  5.353157e-07  9.972832e-07  1.179421e-06   \n",
       "\n",
       "       497.55859375    498.046875  498.53515625   499.0234375  499.51171875   \n",
       "0      9.117289e-09  1.096673e-08  1.372002e-08  5.261354e-09  2.566851e-09  \\\n",
       "1      5.290657e-10  4.596035e-10  7.313141e-10  8.308055e-10  5.550643e-10   \n",
       "2      4.222006e-09  4.185060e-09  7.010891e-09  5.936980e-09  1.659414e-09   \n",
       "3      2.301844e-09  2.628996e-09  2.972329e-09  1.644647e-09  2.010561e-09   \n",
       "4      3.419079e-05  8.060802e-05  9.198100e-05  5.138549e-05  1.361862e-04   \n",
       "...             ...           ...           ...           ...           ...   \n",
       "25039  2.953406e-08  4.402479e-08  3.991601e-08  2.014788e-08  1.905162e-08   \n",
       "25040  1.508944e-06  7.472969e-07  8.472217e-07  1.262639e-06  1.301447e-06   \n",
       "25041  5.055832e-07  7.280705e-07  1.121674e-06  1.241047e-06  7.918726e-07   \n",
       "25042  1.410627e-06  7.396353e-07  7.730455e-07  9.197263e-07  9.801953e-07   \n",
       "25043  6.795099e-07  9.527096e-07  5.708839e-07  6.369546e-07  6.318687e-07   \n",
       "\n",
       "              500.0  \n",
       "0      1.710661e-09  \n",
       "1      2.333624e-10  \n",
       "2      7.369086e-10  \n",
       "3      8.164513e-10  \n",
       "4      1.027223e-04  \n",
       "...             ...  \n",
       "25039  1.079713e-08  \n",
       "25040  6.222116e-07  \n",
       "25041  3.048470e-07  \n",
       "25042  5.919500e-07  \n",
       "25043  3.801804e-07  \n",
       "\n",
       "[25044 rows x 1025 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datos.iloc[: , 2:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se encontraron valores nulos en el conjunto de datos.\n"
     ]
    }
   ],
   "source": [
    "# Verificar si existen valores nulos en el conjunto de datos\n",
    "if data.isnull().values.any():\n",
    "    # Mostrar las columnas con valores nulos\n",
    "    columnas_con_nulos = data.columns[data.isnull().any()].tolist()\n",
    "    print(\"Se encontraron valores nulos en las siguientes columnas:\")\n",
    "    print(columnas_con_nulos)\n",
    "else:\n",
    "    print(\"No se encontraron valores nulos en el conjunto de datos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(data)\n",
    "\n",
    "datos_normalizados = scaler.transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25044, 1025)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.shape(datos_normalizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "x_data = torch.tensor(datos_normalizados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(x_data, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dims = 10\n",
    "num_epochs = 20\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "variational_beta = 1\n",
    "use_gpu = True\n",
    "input_size = data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, input_size//2)\n",
    "        self.linear2 = nn.Linear(input_size//2, input_size//4)\n",
    "        self.linear3 = nn.Linear(input_size//4, input_size//8)\n",
    "        self.linear4 = nn.Linear(input_size//8, input_size//16)\n",
    "        self.fc_mu = nn.Linear(input_size//16, latent_dims)\n",
    "        self.fc_logvar = nn.Linear(input_size//16, latent_dims)\n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x_mu = self.fc_mu(x)\n",
    "        x_logvar = self.fc_logvar(x)\n",
    "        return x_mu, x_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(latent_dims, input_size//16)\n",
    "        self.linear2 = nn.Linear(input_size//16, input_size//8)\n",
    "        self.linear3 = nn.Linear(input_size//8, input_size//4)\n",
    "        self.linear4 = nn.Linear(input_size//4, input_size//2)\n",
    "        self.linear5 = nn.Linear(input_size//2, input_size)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        x = F.relu(self.linear3(x))\n",
    "        x = F.relu(self.linear4(x))\n",
    "        x = torch.sigmoid(self.linear5(x)) # last layer before output is sigmoid, since we are using BCE as reconstruction loss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder2 = Encoder(latent_dims)\n",
    "        self.decoder2 = Decoder(latent_dims)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        latent_mu, latent_logvar = self.encoder2(x)\n",
    "        latent = self.latent_sample(latent_mu, latent_logvar)\n",
    "        x_recon = self.decoder2(latent)\n",
    "        return x_recon, latent_mu, latent_logvar\n",
    "    \n",
    "    def latent_sample(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.empty_like(std).normal_()\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, logvar):\n",
    "    # recon_x is the probability of a multivariate Bernoulli distribution p.\n",
    "    # -log(p(x)) is then the pixel-wise binary cross-entropy.\n",
    "    # Averaging or not averaging the binary cross-entropy over all pixels here\n",
    "    # is a subtle detail with big effect on training, since it changes the weight\n",
    "    # we need to pick for the other loss term by several orders of magnitude.\n",
    "    # Not averaging is the direct implementation of the negative log likelihood,\n",
    "    # but averaging makes the weight of the other loss term independent of the image resolution.\n",
    "    #recon_loss = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "\n",
    "    kldivergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return recon_loss + variational_beta * kldivergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 1398549\n"
     ]
    }
   ],
   "source": [
    "vae = VariationalAutoencoder(latent_dims)\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if use_gpu and torch.cuda.is_available() else \"cpu\")\n",
    "vae = vae.to(device)\n",
    "\n",
    "num_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print('Number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ...\n",
      "Epoch [1 / 10] average reconstruction error: 46744.986428\n",
      "Epoch [2 / 10] average reconstruction error: 46708.277882\n",
      "Epoch [3 / 10] average reconstruction error: 46667.661870\n",
      "Epoch [4 / 10] average reconstruction error: 46663.193793\n",
      "Epoch [5 / 10] average reconstruction error: 46702.655473\n",
      "Epoch [6 / 10] average reconstruction error: 46709.028908\n",
      "Epoch [7 / 10] average reconstruction error: 46751.144770\n",
      "Epoch [8 / 10] average reconstruction error: 46710.511918\n",
      "Epoch [9 / 10] average reconstruction error: 46652.709458\n",
      "Epoch [10 / 10] average reconstruction error: 46703.461630\n",
      "Epoch [11 / 10] average reconstruction error: 46612.671930\n",
      "Epoch [12 / 10] average reconstruction error: 46632.891158\n",
      "Epoch [13 / 10] average reconstruction error: 46655.083785\n",
      "Epoch [14 / 10] average reconstruction error: 46651.684804\n",
      "Epoch [15 / 10] average reconstruction error: 46589.796960\n",
      "Epoch [16 / 10] average reconstruction error: 46629.963882\n",
      "Epoch [17 / 10] average reconstruction error: 46584.757010\n",
      "Epoch [18 / 10] average reconstruction error: 46623.778425\n",
      "Epoch [19 / 10] average reconstruction error: 46585.594786\n",
      "Epoch [20 / 10] average reconstruction error: 46653.224849\n",
      "Epoch [21 / 10] average reconstruction error: 46609.290537\n",
      "Epoch [22 / 10] average reconstruction error: 46585.700270\n",
      "Epoch [23 / 10] average reconstruction error: 46588.235227\n",
      "Epoch [24 / 10] average reconstruction error: 46579.205781\n",
      "Epoch [25 / 10] average reconstruction error: 46771.076117\n",
      "Epoch [26 / 10] average reconstruction error: 46620.697251\n",
      "Epoch [27 / 10] average reconstruction error: 46597.312176\n",
      "Epoch [28 / 10] average reconstruction error: 46579.964091\n",
      "Epoch [29 / 10] average reconstruction error: 46556.133097\n",
      "Epoch [30 / 10] average reconstruction error: 46586.018520\n",
      "Epoch [31 / 10] average reconstruction error: 46556.154850\n",
      "Epoch [32 / 10] average reconstruction error: 46551.086017\n",
      "Epoch [33 / 10] average reconstruction error: 46524.380640\n",
      "Epoch [34 / 10] average reconstruction error: 46590.477893\n",
      "Epoch [35 / 10] average reconstruction error: 46560.735436\n",
      "Epoch [36 / 10] average reconstruction error: 46557.964739\n",
      "Epoch [37 / 10] average reconstruction error: 46606.640356\n",
      "Epoch [38 / 10] average reconstruction error: 46598.548235\n",
      "Epoch [39 / 10] average reconstruction error: 46538.600352\n",
      "Epoch [40 / 10] average reconstruction error: 46529.740733\n",
      "Epoch [41 / 10] average reconstruction error: 46531.581448\n",
      "Epoch [42 / 10] average reconstruction error: 46521.423733\n",
      "Epoch [43 / 10] average reconstruction error: 46511.812425\n",
      "Epoch [44 / 10] average reconstruction error: 46503.757324\n",
      "Epoch [45 / 10] average reconstruction error: 46514.743154\n",
      "Epoch [46 / 10] average reconstruction error: 46511.691491\n",
      "Epoch [47 / 10] average reconstruction error: 46527.431227\n",
      "Epoch [48 / 10] average reconstruction error: 46498.522860\n",
      "Epoch [49 / 10] average reconstruction error: 46530.435502\n",
      "Epoch [50 / 10] average reconstruction error: 46546.939079\n",
      "Epoch [51 / 10] average reconstruction error: 46528.040368\n",
      "Epoch [52 / 10] average reconstruction error: 46621.359490\n",
      "Epoch [53 / 10] average reconstruction error: 46612.671167\n",
      "Epoch [54 / 10] average reconstruction error: 46573.776457\n",
      "Epoch [55 / 10] average reconstruction error: 46509.082679\n",
      "Epoch [56 / 10] average reconstruction error: 46507.940559\n",
      "Epoch [57 / 10] average reconstruction error: 46507.543293\n",
      "Epoch [58 / 10] average reconstruction error: 46494.030079\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     28\u001b[0m \u001b[39m# one step of the optmizer (using the gradients from backpropagation)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     31\u001b[0m train_loss_avg[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()   \u001b[39m#PROBLEMA\u001b[39;00m\n\u001b[1;32m     32\u001b[0m num_batches \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/optim/adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[0;32m--> 141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[1;32m    144\u001b[0m         exp_avgs,\n\u001b[1;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    147\u001b[0m         state_steps,\n\u001b[1;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/optim/adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 281\u001b[0m func(params,\n\u001b[1;32m    282\u001b[0m      grads,\n\u001b[1;32m    283\u001b[0m      exp_avgs,\n\u001b[1;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    286\u001b[0m      state_steps,\n\u001b[1;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/torch/optim/adam.py:393\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m     denom \u001b[39m=\u001b[39m (exp_avg_sq\u001b[39m.\u001b[39msqrt() \u001b[39m/\u001b[39m bias_correction2_sqrt)\u001b[39m.\u001b[39madd_(eps)\n\u001b[0;32m--> 393\u001b[0m param\u001b[39m.\u001b[39;49maddcdiv_(exp_avg, denom, value\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# set to training mode\n",
    "vae.train()\n",
    "\n",
    "train_loss_avg = []\n",
    "\n",
    "print('Training ...')\n",
    "for epoch in range(100):\n",
    "    train_loss_avg.append(0)\n",
    "    num_batches = 0\n",
    "    \n",
    "    for x in data_loader:\n",
    "            #print(type(x))\n",
    "        x = x.to(torch.float32).to(device) # GPU\n",
    "            #opt.zero_grad()\n",
    "\n",
    "        # vae reconstruction\n",
    "        x_recon, latent_mu, latent_logvar = vae(x)\n",
    "        \n",
    "        # reconstruction error\n",
    "        loss = vae_loss(x_recon, x, latent_mu, latent_logvar)\n",
    "        \n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # one step of the optmizer (using the gradients from backpropagation)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss_avg[-1] += loss.item()   #PROBLEMA\n",
    "        num_batches += 1\n",
    "        \n",
    "    train_loss_avg[-1] /= num_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, 10, train_loss_avg[-1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 - SDK v2",
   "language": "python",
   "name": "python310-sdkv2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
